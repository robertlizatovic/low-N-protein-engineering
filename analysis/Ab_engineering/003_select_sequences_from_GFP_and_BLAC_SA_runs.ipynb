{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import random\n",
    "import copy\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../common')\n",
    "import data_io_utils\n",
    "import paths\n",
    "import constants\n",
    "import utils\n",
    "\n",
    "sys.path.append('../A003_policy_optimization/')\n",
    "import models\n",
    "import A003_common\n",
    "\n",
    "import A006_common\n",
    "from unirep import babbler1900 as babbler\n",
    "import sequence_selection\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase\n"
     ]
    }
   ],
   "source": [
    "PROTEIN = 'BLAC' # 'GFP' or 'BLAC'\n",
    "\n",
    "if PROTEIN == 'GFP':\n",
    "    WT_SEQ = constants.AVGFP_AA_SEQ\n",
    "    data_dir = os.path.join(data_io_utils.S3_DATA_ROOT, 'chip_1/simulated_annealing/GFP')\n",
    "elif PROTEIN == 'BLAC':\n",
    "    WT_SEQ = constants.BETA_LAC_AA_SEQ\n",
    "    data_dir = os.path.join(data_io_utils.S3_DATA_ROOT, 'chip_1/simulated_annealing/beta_lactamase')\n",
    "    \n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sync data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_s3_path_to_local(data_dir) # Very large sync ~300GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_s3_path_to_local(paths.EVOTUNING_CKPT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniRep utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAC\n"
     ]
    }
   ],
   "source": [
    "UNIREP_BATCH_SIZE = 3500\n",
    "\n",
    "TOP_MODEL_ENSEMBLE_NMEMBERS = models.ENSEMBLED_RIDGE_PARAMS['n_members']\n",
    "TOP_MODEL_SUBSPACE_PROPORTION = models.ENSEMBLED_RIDGE_PARAMS['subspace_proportion']\n",
    "TOP_MODEL_NORMALIZE = models.ENSEMBLED_RIDGE_PARAMS['normalize']\n",
    "TOP_MODEL_PVAL_CUTOFF = models.ENSEMBLED_RIDGE_PARAMS['pval_cutoff']\n",
    "\n",
    "if PROTEIN == 'GFP':\n",
    "    print('GFP')\n",
    "    def load_base_model(model_name):\n",
    "        if model_name == 'ET_Global_Init_1':\n",
    "            base_model = babbler(batch_size=UNIREP_BATCH_SIZE, model_path=paths.GFP_ET_GLOBAL_INIT_1_WEIGHT_PATH)\n",
    "            print('Loading weights from:', paths.GFP_ET_GLOBAL_INIT_1_WEIGHT_PATH)\n",
    "        elif model_name == 'ET_Global_Init_2':\n",
    "            base_model = babbler(batch_size=UNIREP_BATCH_SIZE, model_path=paths.GFP_ET_GLOBAL_INIT_2_WEIGHT_PATH)\n",
    "            print('Loading weights from:', paths.GFP_ET_GLOBAL_INIT_2_WEIGHT_PATH)\n",
    "        elif model_name == 'ET_Random_Init_1':\n",
    "            base_model = babbler(batch_size=UNIREP_BATCH_SIZE, model_path=paths.GFP_ET_RANDOM_INIT_1_WEIGHT_PATH)\n",
    "            print('Loading weights from:', paths.GFP_ET_RANDOM_INIT_1_WEIGHT_PATH)\n",
    "        elif model_name =='OneHot':\n",
    "            # Just need it to generate one-hot reps.\n",
    "            # Top model created within OneHotRegressionModel doesn't actually get used.\n",
    "            base_model = models.OneHotRegressionModel('EnsembledRidge') \n",
    "        else:\n",
    "            assert False, 'Unsupported base model'\n",
    "\n",
    "        return base_model\n",
    "    \n",
    "elif PROTEIN == 'BLAC':\n",
    "    print('BLAC')\n",
    "    class BetaLacOneHotEncoder(object):\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def encode_seqs(self, seqs):\n",
    "            return utils.encode_aa_seq_list_as_matrix_of_flattened_one_hots(seqs)\n",
    "    \n",
    "    def load_base_model(model_name):\n",
    "        if model_name == 'ET_Global_Init_1':\n",
    "            base_model = babbler(batch_size=UNIREP_BATCH_SIZE, model_path=paths.BLAC_ET_GLOBAL_INIT_1_WEIGHT_PATH)\n",
    "            print('Loading weights from:', paths.BLAC_ET_GLOBAL_INIT_1_WEIGHT_PATH)\n",
    "        elif model_name == 'ET_Random_Init_1':\n",
    "            base_model = babbler(batch_size=UNIREP_BATCH_SIZE, model_path=paths.BLAC_ET_RANDOM_INIT_1_WEIGHT_PATH)\n",
    "            print('Loading weights from:', paths.BLAC_ET_RANDOM_INIT_1_WEIGHT_PATH)\n",
    "        elif model_name =='OneHot':\n",
    "            # Just need it to generate one-hot reps.\n",
    "            # Doing it this way to be consistent with the GFP pipeline\n",
    "            base_model = BetaLacOneHotEncoder()\n",
    "        else:\n",
    "            assert False, 'Unsupported base model'\n",
    "\n",
    "        return base_model\n",
    "\n",
    "# Generate representations\n",
    "def generate_reps(seq_list, base_model, sess):        \n",
    "    if 'babbler1900' == base_model.__class__.__name__:\n",
    "        assert len(seq_list) <= UNIREP_BATCH_SIZE\n",
    "        hiddens = base_model.get_all_hiddens(seq_list, sess)\n",
    "        rep = np.stack([np.mean(s, axis=0) for s in hiddens],0)\n",
    "\n",
    "    else: # one hot model\n",
    "        rep = base_model.encode_seqs(seq_list)\n",
    "\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sr_vs_nsr_pred_plot(nsr_yhat_wt, sr_yhat_wt, nsr_yhat_top, sr_yhat_top, output_dir):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(sr_yhat_top, nsr_yhat_top, '.k')\n",
    "    plt.axvline(sr_yhat_wt, color='r')\n",
    "    plt.axhline(nsr_yhat_wt, color='r')\n",
    "\n",
    "    plt.text(x=0.6, y=0.9,  s='SR predicted top seqs >WT: %d' % np.sum(sr_yhat_top > sr_yhat_wt),\n",
    "            horizontalalignment='right', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "    plt.text(x=0.6, y=0.8,  s='NSR predicted top seqs >WT: %d' % np.sum(nsr_yhat_top > nsr_yhat_wt),\n",
    "        horizontalalignment='right', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "\n",
    "    plt.xlabel('Predicted qfunc, original sparse refit top model')\n",
    "    plt.ylabel('Predicted qfunc, non sparse refit top model')\n",
    "\n",
    "    out_file = os.path.join(output_dir, 'sr_vs_nsr_plot.png')\n",
    "    plt.savefig(out_file, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "\n",
    "def top_seq_and_traj_plot(fit_mat, trajectory_indices_yielding_top_seqs, \n",
    "                          seq_indices_inside_top_trajectories, output_dir):\n",
    "    fit_mat = res_sa['fitness_history']\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    for i in range(3):\n",
    "        plt.plot(fit_mat[:, trajectory_indices_yielding_top_seqs[i]], '-')\n",
    "        plt.plot(seq_indices_inside_top_trajectories[i], \n",
    "                 fit_mat[seq_indices_inside_top_trajectories[i], trajectory_indices_yielding_top_seqs[i]], '.k')\n",
    "        \n",
    "    out_file = os.path.join(output_dir, 'top_seq_and_traj_plot.png')\n",
    "    plt.savefig(out_file, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def qfunc_hist_plot(wt_qfunc, init_fitness, top_seq_fitness, output_dir):\n",
    "    init_fitness[np.isinf(init_fitness)] = 0 \n",
    "    top_seq_fitness[np.isinf(top_seq_fitness)] = 0 \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.hist(init_fitness, bins=50, color='b', alpha=0.3)\n",
    "    plt.hist(top_seq_fitness, bins=50, color='r', alpha=0.3)\n",
    "    plt.axvline(wt_qfunc, color='k')\n",
    "    \n",
    "    plt.text(x=0.6, y=0.9,  s='Num initial seqs >WT: %d' % np.sum(init_fitness > wt_qfunc),\n",
    "        horizontalalignment='right', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "    plt.text(x=0.6, y=0.8,  s='Num optimized seqs >WT: %d' % np.sum(top_seq_fitness > wt_qfunc),\n",
    "        horizontalalignment='right', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "    \n",
    "    out_file = os.path.join(output_dir, 'qfunc_hist_plot.png')\n",
    "    plt.savefig(out_file, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "def seq_dist_summary_plots(top_seqs, top_seq_fitness, top_seq_fitness_ensemble, wt_qfunc, output_dir):\n",
    "    ref_seq = WT_SEQ\n",
    "    \n",
    "    pct = np.percentile(top_seq_fitness_ensemble, [5, 95], axis=1).T\n",
    "    errb = pct - top_seq_fitness.reshape((-1,1))\n",
    "    errb[:,0] = -errb[:,0]\n",
    "\n",
    "    ld_mat = utils.levenshtein_distance_matrix(top_seqs)\n",
    "    ld_sidx = np.argsort(-np.mean(ld_mat, axis=0))\n",
    "    ld_mat[ld_mat == 0] = np.inf\n",
    "    ld_mat = ld_mat[ld_sidx]\n",
    "    ld_mat = ld_mat[:, ld_sidx]\n",
    "\n",
    "    fr = np.max(top_seq_fitness) - np.min(top_seq_fitness)\n",
    "\n",
    "    fig, (a0, a1, a2) = plt.subplots(3, 1, gridspec_kw={'height_ratios': [1,1,4],'hspace':0.025}, figsize=(2*5,2*7.25))\n",
    "    a0.errorbar(np.arange(len(top_seq_fitness)), top_seq_fitness[ld_sidx], yerr=errb[ld_sidx].T, fmt='.k', zorder=-1)\n",
    "    a0.axhline(wt_qfunc, color='r')\n",
    "    a0.set_xlim([0-0.5, len(top_seq_fitness)-0.5])\n",
    "    a0.set_xticks([])\n",
    "    a0.set_ylabel('predicted\\nfitness')\n",
    "    a0.set_ylim([0, 1.5])\n",
    "    a0.grid('on')\n",
    "\n",
    "    ld_ref = utils.levenshtein_distance_matrix([ref_seq], top_seqs).reshape(-1)\n",
    "    a1.bar(np.arange(len(ld_ref)), ld_ref[ld_sidx], width=1, color='gray')\n",
    "    #a1.set_ylim(bottom=np.min(top_seq_fitness)-0.05*fr)\n",
    "    a1.set_xlim([0-0.5, len(top_seq_fitness)-0.5])\n",
    "    a1.set_xticks([])\n",
    "    a1.set_ylabel('num. mutations\\nto reference')\n",
    "    a1.grid('on')\n",
    "\n",
    "\n",
    "    im = a2.imshow(ld_mat, aspect='auto')\n",
    "    a2.set_ylabel('sequence')\n",
    "    a2.set_xlabel('sequence')\n",
    "\n",
    "    cb_ax = fig.add_axes([0.93, 0.23, 0.02, 0.3])\n",
    "    cbar = fig.colorbar(im, cax=cb_ax)\n",
    "    cbar.ax.get_yaxis().labelpad = 15\n",
    "    cbar.ax.set_ylabel('num. mutations', rotation=270)\n",
    "    \n",
    "    out_file = os.path.join(output_dir, 'seq_similarity_summary_plot.png')\n",
    "    plt.savefig(out_file, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    m = ld_mat\n",
    "    m[m == np.inf] = 0\n",
    "    uv,uc = np.unique(ld_mat.reshape(-1), return_counts=True)\n",
    "    plt.bar(uv, uc)\n",
    "    plt.title('Pairwise levenshtein distance distribution\\nMedian pw lev dist=%d'%np.median(ld_mat.reshape(-1)))\n",
    "    \n",
    "    out_file = os.path.join(output_dir, 'pairwise_levenshtein_distance_plot.png')\n",
    "    plt.savefig(out_file, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main sequence selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_results(res_file):\n",
    "    with open(res_file, 'rb') as f:\n",
    "        res = pickle.load(f)\n",
    "        \n",
    "    res_sa = sequence_selection.convert_result_vals_to_mat(res['sa_results'])\n",
    "    return res, res_sa\n",
    "    \n",
    "\n",
    "def select_top_seqs(res_file, nseq_select, burnin=250, max_sa_itr=None):\n",
    "    print('SELECTION')\n",
    "    res_file_name = os.path.basename(res_file)\n",
    "        \n",
    "    print(res_file)\n",
    "    print('Loading results and converting SA histories to numpy arrays')\n",
    "    res, res_sa = load_results(res_file)\n",
    "    fit_mat = res_sa['fitness_history']\n",
    "\n",
    "    init_fitness = fit_mat[0,:]\n",
    "\n",
    "    print('Selecting top sequences')\n",
    "    # First identify the best sequence in each SA trajectory.\n",
    "    top_seqs, top_seq_fitness, _, top_seq_idx = sequence_selection.get_best_sequence_in_each_trajectory(\n",
    "            res_sa, burnin=burnin, max_sa_itr=max_sa_itr)\n",
    "\n",
    "    # Now, select the top seqs of the best-in-trajectory sequences. \n",
    "    # These are are our official selections!\n",
    "    # top_seq_idx is an index for each trajectory that says where in the trajectory the best sequence is. \n",
    "    sidx = np.argsort(-top_seq_fitness)\n",
    "    top_sidx = sidx[:nseq_select]\n",
    "\n",
    "    trajectory_indices_yielding_top_seqs = top_sidx\n",
    "    seq_indices_inside_top_trajectories = top_seq_idx[top_sidx]\n",
    "    selected_top_seqs = top_seqs[top_sidx] ## official selection\n",
    "    selected_top_seq_fitness = top_seq_fitness[top_sidx] ## official selection\n",
    "    selected_top_ensemble_fitness_preds = []\n",
    "    for i in range(len(trajectory_indices_yielding_top_seqs)):\n",
    "        selected_top_ensemble_fitness_preds.append(\n",
    "            res_sa['fitness_mem_pred_history'][seq_indices_inside_top_trajectories[i]][\n",
    "                trajectory_indices_yielding_top_seqs[i]]\n",
    "        )\n",
    "        \n",
    "    # Turn these selections into a dataframe\n",
    "    id_prefix = res_file_name.replace('.p', '')\n",
    "    fit_mat_idx = [str(s[0]) + '_' + str(s[1]) for s in list(zip(*[list(seq_indices_inside_top_trajectories), \n",
    "           list(trajectory_indices_yielding_top_seqs)]))]\n",
    "    seq_ids = [id_prefix + '-seq_idx_' + fmi for fmi in fit_mat_idx]\n",
    "    \n",
    "    select_df = pd.DataFrame()\n",
    "    select_df['id'] = seq_ids\n",
    "    select_df['seq_idx'] = seq_indices_inside_top_trajectories # row idx of res_sa['fitness_history']\n",
    "    select_df['trajectory_idx'] = trajectory_indices_yielding_top_seqs # col idx of res_sa['fitness_history']\n",
    "    select_df['predicted_fitness'] = selected_top_seq_fitness\n",
    "    select_df['ensemble_predicted_fitness'] = selected_top_ensemble_fitness_preds\n",
    "    select_df['seq'] = selected_top_seqs\n",
    "    \n",
    "    return select_df, res, res_sa\n",
    "\n",
    "def validate_top_seqs(select_df, output_dir, res, res_sa, burnin=250, max_sa_itr=None):\n",
    "    fit_mat = res_sa['fitness_history']\n",
    "    seq_mat = res_sa['seq_history']\n",
    "    \n",
    "    print('VALIDATION')\n",
    "    trajectory_indices_yielding_top_seqs = np.array(select_df['trajectory_idx'])\n",
    "    seq_indices_inside_top_trajectories = np.array(select_df['seq_idx'])\n",
    "    selected_top_seq_fitness = np.array(select_df['predicted_fitness'])\n",
    "    top_seq_fitness_ensemble = np.stack(select_df['ensemble_predicted_fitness'])\n",
    "    selected_top_seqs = np.array(select_df['seq'])\n",
    "    \n",
    "\n",
    "    # First check that after all the manipulation we did, that manually extracting the\n",
    "    # sequence and its fitness based on the identified indices lines up with the what\n",
    "    # the selection code provides.\n",
    "    print('Validating sequence selection doing a manual re-extraction')\n",
    "    for i in range(len(trajectory_indices_yielding_top_seqs)):\n",
    "        man_sel_fitness_ens = res_sa['fitness_mem_pred_history'][\n",
    "            seq_indices_inside_top_trajectories[i]][trajectory_indices_yielding_top_seqs[i]]\n",
    "        man_sel_fitness = fit_mat[seq_indices_inside_top_trajectories[i], \n",
    "                                  trajectory_indices_yielding_top_seqs[i]]\n",
    "        man_sel_seq = seq_mat[seq_indices_inside_top_trajectories[i], \n",
    "                              trajectory_indices_yielding_top_seqs[i]]\n",
    "        \n",
    "        assert man_sel_fitness ==  selected_top_seq_fitness[i]\n",
    "        assert man_sel_seq == selected_top_seqs[i]\n",
    "        \n",
    "    # Re-calculate the ensemble's prediction and make sure it lines up with the extracted\n",
    "    # fitness.\n",
    "    recalc_pred_fitness = np.mean(np.stack(select_df['ensemble_predicted_fitness']), axis=1)\n",
    "    assert np.allclose(recalc_pred_fitness - selected_top_seq_fitness, 0, atol=1e-5)\n",
    "    \n",
    "    # Validate these sequences are good. Score again with the original sparse refit \n",
    "    # top model as well as the non-sparse refit model\n",
    "    print('Rescoring sequences')\n",
    "    print('\\tLoading base model')\n",
    "    tf.reset_default_graph()\n",
    "    base_model_name = res_file.split('-')[1]\n",
    "    if base_model_name == 'LargeMut':\n",
    "        base_model_name =  res_file.split('-')[2]\n",
    "    base_model = load_base_model(base_model_name)\n",
    "\n",
    "    train_df = res['train_df']\n",
    "    train_seqs = list(train_df['seq'])\n",
    "    train_qfunc = np.array(train_df['quantitative_function'])\n",
    "\n",
    "    # Generate reps for the sequences\n",
    "    print('\\tGenerating reps')\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        train_reps = generate_reps(train_seqs, base_model, sess)\n",
    "        top_seq_reps = generate_reps(list(selected_top_seqs), base_model, sess)\n",
    "        wt_seq_rep = generate_reps([WT_SEQ], base_model, sess)\n",
    "\n",
    "    # Train an NSR top model\n",
    "    print('\\tTraining an NSR top model')\n",
    "    nsr_top_model = A003_common.train_ensembled_ridge(\n",
    "        train_reps, \n",
    "        train_qfunc, \n",
    "        n_members=TOP_MODEL_ENSEMBLE_NMEMBERS, \n",
    "        subspace_proportion=TOP_MODEL_SUBSPACE_PROPORTION,\n",
    "        normalize=TOP_MODEL_NORMALIZE, \n",
    "        do_sparse_refit=False, \n",
    "        pval_cutoff=TOP_MODEL_PVAL_CUTOFF\n",
    "    )\n",
    "\n",
    "    sr_top_model = res['top_model']\n",
    "\n",
    "    # Score WT seqs\n",
    "    nsr_yhat_wt = nsr_top_model.predict(wt_seq_rep)\n",
    "    sr_yhat_wt = sr_top_model.predict(wt_seq_rep)\n",
    "\n",
    "    # Score the the top sequences.\n",
    "    nsr_yhat_top = nsr_top_model.predict(top_seq_reps)\n",
    "    sr_yhat_top = sr_top_model.predict(top_seq_reps)\n",
    "\n",
    "    # First make sure that the freshly predicted fitness of the top seqs match the recorded ones.\n",
    "    assert np.corrcoef(sr_yhat_top, selected_top_seq_fitness)[0,1] > 0.99\n",
    "\n",
    "    print('Generating validation plots')\n",
    "    ## Now generate a bunch of plots\n",
    "    sr_vs_nsr_pred_plot(nsr_yhat_wt, sr_yhat_wt, nsr_yhat_top, sr_yhat_top, output_dir)\n",
    "    top_seq_and_traj_plot(fit_mat, trajectory_indices_yielding_top_seqs, \n",
    "            seq_indices_inside_top_trajectories, output_dir)\n",
    "    \n",
    "    # all fitnesses for best-in-trajectory sequences\n",
    "    _, all_top_seq_fitness, _, _ = sequence_selection.get_best_sequence_in_each_trajectory(\n",
    "            res_sa, burnin=burnin, max_sa_itr=max_sa_itr)\n",
    "    qfunc_hist_plot(sr_yhat_wt, fit_mat[0], all_top_seq_fitness, output_dir)\n",
    "    \n",
    "    seq_dist_summary_plots(list(selected_top_seqs), selected_top_seq_fitness, \n",
    "                          top_seq_fitness_ensemble, sr_yhat_wt, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the sequence selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BURNIN = 250\n",
    "MAX_SA_ITR = None # if None use all of them.\n",
    "NSEQ_SELECT = 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "result_files = sorted(glob.glob(os.path.join(data_dir, PROTEIN + '_SimAnneal*.p')))\n",
    "\n",
    "# Special case globs\n",
    "#result_files = sorted(glob.glob(os.path.join(data_dir, '*SparseRefit_False*.p')))\n",
    "\n",
    "print(len(result_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0024-00-3e721641-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0024-01-3a0e3d4-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0024-02-31e54146-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0024-03-3764e943-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0024-04-4502d3-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0096-00-3ea5f6e-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0096-01-2db7371-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0096-02-341cf5c-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0096-03-12da09f-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Global_Init_1-0096-04-30cb4ce7-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0024-00-17cbc6c3-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0024-01-25f01ed8-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0024-02-f795278-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0024-03-2a95c17b-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0024-04-36079a2c-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0096-00-46b57ab-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0096-01-3d7813de-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0096-02-1f121a71-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0096-03-33bfb65e-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-ET_Random_Init_1-0096-04-35083dba-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-LargeMut-ET_Global_Init_1-0096-00-2bc1d82f-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-LargeMut-ET_Random_Init_1-0096-00-21e3a907-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-LargeMut-OneHot-0096-00-75e84ab-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0024-00-15ce00a7-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0024-01-15947f78-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0024-02-1d0c4863-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0024-03-1931926-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0024-04-1989c098-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0096-00-2277bff5-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0096-01-bb13a0-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0096-02-311cbde3-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0096-03-108b3ec7-selected_seqs/selected_seqs_df.pkl\n",
      "\n",
      "Already done: /notebooks/analysis/common/../../data/s3/chip_1/simulated_annealing/beta_lactamase/BLAC_SimAnneal-OneHot-0096-04-1a4d5748-selected_seqs/selected_seqs_df.pkl\n"
     ]
    }
   ],
   "source": [
    "for res_file in result_files:\n",
    "    print()\n",
    "    output_dir = res_file.replace('.p', '-selected_seqs')\n",
    "    output_file = os.path.join(output_dir, 'selected_seqs_df.pkl')\n",
    "    \n",
    "    if not os.path.exists(output_file):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        select_df, res, res_sa = select_top_seqs(res_file, NSEQ_SELECT, burnin=BURNIN, max_sa_itr=MAX_SA_ITR)\n",
    "        validate_top_seqs(select_df, output_dir, res, res_sa, burnin=BURNIN, max_sa_itr=MAX_SA_ITR)\n",
    "\n",
    "        select_df.to_pickle(output_file)\n",
    "    else:\n",
    "        print('Already done:', output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_io_utils.sync_local_path_to_s3(data_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
